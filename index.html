<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="无病呻吟...">
<meta property="og:type" content="website">
<meta property="og:title" content="少年游">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="少年游">
<meta property="og:description" content="无病呻吟...">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="少年游">
<meta name="twitter:description" content="无病呻吟...">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>少年游</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">少年游</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-主页"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-标签"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-分类"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-文章"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/13/R-CNN算法学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://oyvr3xxmh.bkt.clouddn.com/17-11-4/36375047.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="少年游">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/13/R-CNN算法学习/" itemprop="url">R-CNN算法学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-13T21:06:37+08:00">
                2018-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/note/" itemprop="url" rel="index">
                    <span itemprop="name">note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>&emsp;&emsp;R-CNN提出之前的几年里，在权威数据集PASCAL VOC上，目标检测的表现已经趋于稳定。表现最好的方法是融合了多种图像低维特征和高维上下文环境的复杂系统。而在R-CNN这篇论文中，提出了一个简单易扩展的目标检测算法，其效果将mAP指标（评价一个目标检测算法的指标）提升30%。这个方法主要秉持了两个观点：1.为了实现目标检测和语义分割，将大型卷积神经网络用于图像的候选区域；2.因为带标签的训练集较少，故先针对辅助任务进行了一个有监督的预训练，再基于此对特征任务进行微调，这在实验中取得了很好的效果。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h4><p>&emsp;&emsp;论文在开头第一句便提出：特征很重要。在2010-2012年，针对PASCAL VOC权威数据集的物体检测发展是十分缓慢的，所做的工作也仅仅是整合几个不同的系统进行检测，或者是对之前成功的方法做一些不重要的改变。过去所使用的方法均是基于SIFT和HOG（这两种方法都是基于图像中梯度方向直方图的特征提取法）。在文章中说到这种表示和V1层（灵长类动物视觉通道的第一块皮质区域）的表示方法类似。但是我们也能知道计算特征是一个分层的多阶段过程，这样才能得到更富有信息的特征。</p>
<p>&emsp;&emsp;模式识别最早的尝试是Fukushima的neocognitron，这是一个受生物启发的多层次，平移不变性的模型，但当时缺少有监督的训练算法。之后LeCun等人证明了可以通过基于反向传播的随机梯度下降算法训练卷积神经网络，并取得了不错的效果（这个便是LeCun当时提出的LeNet5），而该模型被认为是继承了neocognitron的模型。</p>
<p>&emsp;&emsp;CNN模型在1990年代被广泛使用，但是之后由于SVM的出现和成功逐渐淡出人们的视野。而在2012年AlexNet在ImageNet图片分类比赛中的大成功，卷积网络又重新大热，而该模型的成功和120万带标记的图片密不可分，同时该模型和LeNet5的架构非常相似（不同之处在于运用了ReLU非线性激活和Dropout对抗过拟合等，这些可以在之前的文章中看到）。而ImageNet比赛的成功也使Object Detection领域有了新的讨论：即该如何使图像分类比赛的结果泛化到PASCAL VOC比赛中。</p>
<p>&emsp;&emsp;该论文回答了上述问题，并展示了一个卷积神经网络是如何使目标检测得到更好的性能，作为对比的是传统的基于简单的HOG特征形成的检测系统。为了达到这个效果，论文主要针对的是两个问题：第一个是用深度网络进行目标定位，第二个则是训练一个只用少量的带标签数据的大型模型（因为在目标检测领域的带标签的数据集较少）。</p>
<p>&emsp;&emsp;不同于图像分类，目标检测需要在一张图片中定位一个或多个目标。一种方法是将定位问题视作回归问题，但效果并不好。另一种替代的方法是建立一个滑动窗口检测器，而CNNs在过去的20年均是用的这个方法，主要实在特定的目标类别上（比如脸和行人）。为了保持高的分辨率，这些CNNs通常只使用很小的卷积网络（两层卷积层池化层），因为如果层数过高分辨率又高参数的数量将是极大的。在该论文的工作中也使用了而该方法，但是网络的层数是五层，这就导致该网络在输入的图片上有非常大的感受野（195x195pixels）和步长（32x32pexels），这样做让滑动窗口精确定位成为了一个技术开发上的挑战。</p>
<p>&emsp;&emsp;R-CNN采用了“recognition using regions”方法来解决CNN的定位问题，该方法在目标检测和语义分割问题都取得了成功。测试阶段，该方法针对输入图生成了大约2000个独立类别的候选区域（region proposals），再通过一个CNN从每个区域提取出一个固定长度的特征向量，最后通过一个特定类别的线性SVMs来对每一个候选区域进行分类。为了保证CNN输入的尺寸固定，采用了一个简单的技术（affine image warping），不用管候选区域的尺寸。下图展示了R-CNN算法的一个大致步骤和结果。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/77327248.jpg" alt=""><br>&emsp;&emsp;在检测方面另一个重要的挑战是带标签的训练集过于稀少，因而不足以训练一个大型的卷积神经网络模型。一种通用的解决方案是使用无监督的预训练，然后采用有监督性的微调（有点类似迁移学习）。即将ILSVRC作为辅助数据集，训练得到模型参数，然后将该模型迁移到PASCAL数据集上并进行一些微调，这是一种在数据较少情况下有效学习高维CNN特征的方法。</p>
<p>&emsp;&emsp;该系统同样非常的高效，唯一特定类型的计算表示相对小型的矩阵向量的点乘和贪婪非极大值抑制。在这里介绍一下非极大值抑制，因为在上述说明中滑动窗口会产生很多的候选区域，比如一个人的脸可能会有很多框（每个框带有一个分类器的得分），而非极大值抑制要做的便是针对这张脸，只保留一个最优的框，抑制的过程是一个迭代-遍历-消除的过程。</p>
<p>&emsp;&emsp;具体算法细节可以分成三步：1.将所有框的得分排序，选中最高分框。2.遍历其余所有框，如果和当前最高分框的重叠面积（IOU）大于一定的阈值就删除（这样做的目的便是去除对某一张脸的最高分框以外的框）。3.从未处理的框中继续选择得分最高的框重复操作。</p>
<h4 id="2-使用R-CNN进行目标检测"><a href="#2-使用R-CNN进行目标检测" class="headerlink" title="2 使用R-CNN进行目标检测"></a>2 使用R-CNN进行目标检测</h4><p>&emsp;&emsp;R-CNN目标检测系统总共包含三个模块：1.生成类别独立的候选区域；2.一个大型的卷积神经网络模型，针对候选区域提取出固定尺寸的特征向量；3.一系列特定类别的线性SVMs。</p>
<p>&emsp;&emsp;在当时提出了很多关于如何选取候选区域的方法，该系统最终选择了selective search方法。特征提取方面该系统最终针对每个候选区域提取出一个4096维的特征向量，所运用的神经网络模型便是AlexNet。在对输入图片尺寸设置上，因为喂给卷积神经网络的图片都需要227x227，而选择出来的候选区域的大小不一。在这里选择了一种最简单的实现方法，即不论候选区域图片大小或横纵比是多少都直接将其不再保持原本横纵比缩放到指定尺寸。</p>
<h4 id="2-1-测试"><a href="#2-1-测试" class="headerlink" title="2.1 测试"></a>2.1 测试</h4><p>&emsp;&emsp;在测试的过程中，首先运用selective search在测试图片上提取出约2000个候选区域，并将每个都前向传播进CNN中，之后便是分类然后进行贪婪的非极大值抑制来选取出最优的detection。</p>
<p>&emsp;&emsp;关于性能的分析，两个性质使检测效率非常高：1.因为卷积神经网络参数在所有类别都共享（相对全连接网络减少了很多的参数量）；2.特征向量维度不高（相比于其它的系统）。最终针对一张图片计算出每个候选区域特征向量的时间是（在GPU上每张图片13s，CPU上53s），因为共享是消耗均摊到每一个类别上了，而分类的计算主要是特征向量和SVM权重之间矩阵的点乘和非极大值抑制方法的计算。特征向量矩阵是一个2000x4096，SVM的权重矩阵为4096xN（N为类别的数量）。</p>
<p>&emsp;&emsp;根据上述的分析可以看到，R-CNN系统完全可以扩展到数千个种类物体的检测，并且不需要使用类似散列表这样的技术。甚至当有10W类别时，最后结果矩阵的点乘在当时一个先进的多核CPU下也只需要10s，这种高效率不仅仅是使用候选区域和共享特征。</p>
<h5 id="2-2-训练"><a href="#2-2-训练" class="headerlink" title="2.2 训练"></a>2.2 训练</h5><p>&emsp;&emsp;在训练的过程中，针对所有候选区域，选取了IoU&gt;=0.5（就是和对比框重叠占的比例）的作为与当前样本同一种分类，会进行剔除。卷积神经网络采用的随机梯度下降并设置学习速率为0.001.针对IoU的设置可以采用验证集来看哪一个更好。<br>最终的结果如下表所示，该结果是在PASCAL VOC 2010数据集上得到的。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/30454736.jpg"><br>&emsp;&emsp;从表中可以看到，论文将R-CNN模型和当时四个较强的系统进行了对比。在这里提到了使用直方图交叉核的SVM对比于UVA系统中的多特征非线性核的SVM在mAP上从35.1%提高到了53.7%并且运行地更加快速。</p>
<h4 id="3-可视化，ablation和错误模式"><a href="#3-可视化，ablation和错误模式" class="headerlink" title="3 可视化，ablation和错误模式"></a>3 可视化，ablation和错误模式</h4><h5 id="3-1-可视化学习到的特征"><a href="#3-1-可视化学习到的特征" class="headerlink" title="3.1 可视化学习到的特征"></a>3.1 可视化学习到的特征</h5><p>&emsp;&emsp;通常第一层卷积核得到的特征都能被直接观测并易于理解，它们大多抓取到的都是边缘和对比度。而后面层数捕获的特征的理解更具挑战性，在这里论文提出了一种简单的无参数的方法来直接展示深层网络究竟学到了什么。</p>
<p>&emsp;&emsp;该方法的主要思想是首先从网络中甄选出一个特定的单元（特征），并将其用作自己的目标检测器。这句话的意思就是说，我们再大量的候选区域（约为1千万）中计算上述选到的单元的激活值，即将候选区域输入进该单元得到的结果。然后将得到的结果排序并运用非极大值抑制方法进行剔除最终留下那些最高得分的候选区域。这样做便是让被选择的单元自己说话，因为通过观察排序出来最高的那些候选区域可以看到这个单元主要是在捕获那些东西。</p>
<p>&emsp;&emsp;论文针对了卷积神经网络第五层的最大池化层进行了可视化操作，该池化层的feature map是6x6x256，忽略边界效应每一个池化单元（pooling filter）在输入图片上的感受野是195x195（中心的能感受到几乎全部，边缘的则是经过裁剪的较小的感受野）。下面展示了该池化层可视化后的结果，每一行代表了一个池化单元经过非极大值抑制处理后最高的16个得分。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/70676408.jpg"><br>&emsp;&emsp;从上图中能够看到，第一行更多的是在学习人脸，第二行在学习狗和点阵列，第三行在学习红色斑点，还有诸如学习文本三角结构等之类的特征。在该层之后的全连接层则具有将这些丰富特征的大量组合进行建模的能力。</p>
<h5 id="3-2-Ablation-studies"><a href="#3-2-Ablation-studies" class="headerlink" title="3.2 Ablation studies"></a>3.2 Ablation studies</h5><p>&emsp;&emsp;关于Ablation study是什么，在这里引用一下一种解释：An ablation study typically refers to removing some “feature” of the model or algorithm, and seeing how that affects performance，即它想要做的便是看看哪些特征在模型或者算法上表现是最好的。</p>
<p>&emsp;&emsp;在这里首先拿没有进过微调的AlexNet模型，将其直接运用到PASCAL数据集上得到结果，然后将其与经过微调的模型得到的结果对比，具体结果如下所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/2438236.jpg"><br>&emsp;&emsp;上表第一道第三行所示便是没有经过微调模型得到的结果，可以看到第七层全连接层得到的结果比第六层的结果还要差，这表明了该模型29%的参数可以删去（从以前的文章中我们能够知道早期的卷积神经网络模型大部分的参数都在最后几层全连接层中，所以才会有后来的avg pooling layer代替fully connected layer）。更加震惊的是把第六第七层全删掉得到的结果也很好，所以说CNN大部分的表示能力来自于卷积层而非全连接层，故对于滑动窗口检测器来说完全可以只需要该模型的前五层。</p>
<p>&emsp;&emsp;从第四到第六行可以看到在经过微调参数后，性能提升地非常明显，而且对于第六层和第七层的提升更大，这表明了从第五层pool中学到的都是一些通用的，大部分的提升来自于学习领域特定的非线性分类器结果。</p>
<h5 id="3-2-检测错误分析"><a href="#3-2-检测错误分析" class="headerlink" title="3.2 检测错误分析"></a>3.2 检测错误分析</h5><p>&emsp;&emsp;在这里使用了Hoiem等人的检测分析工具对结果检测得到了如下所示的图：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/69399516.jpg"><br>&emsp;&emsp;上面每一张图代表了一个不断演变的FP类型的分布（注意这里FP是分类错误即本来是负样例被分类成正样例，具体见下图）。FP分为四种类型：Loc（定位的准确率低，检测框和真实值的覆盖率在0.1到0.5或者存在重复值），Sim（与相似类别混淆），Oth（与不相似的类别混淆），BG（检测框标记在了背景上）。从上图中能看到该系统主要的错误结果是因为定位的准确率低，第三列显示了使用简单的边界回归方法修复了许多定位的错误。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/69991674.jpg"><br>&emsp;&emsp;基于错误分析，R-CNN试验了一种简单的方法来降低定位错误，该方法受DPM模型中的检测框回归方法的启发，使用在候选区域提取出的pool5(第五层卷积层)的特征训练了一个线性回归模型，来修正出一个更加精确的检测框。</p>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h4><p>R-CNN可以算作第一个把卷积神经网络引入object detection领域并取得较好成功的模型。<strong>该模型有几个亮点：</strong></p>
<ul>
<li>第一个是速度降低了很多，因为它选择了约2000个候选区域之后的特征向量提取和类别判断都是基于这些候选区域；</li>
<li>第二个是解决了训练集稀少情况下如何通过使用辅助数据集（该论文使用了ILSVC数据集）加上参数微调来较好地完成任务。</li>
</ul>
<p><strong>最后再次总结下R-CNN算法的总体步骤：</strong></p>
<ol>
<li>通过Selective Search算法对一张输入图像筛选出2000个左右的候选区域；</li>
<li>对每个候选区域使用AlexNet网络提取出特征向量；</li>
<li>将每个特征向量送入所有SVM的线性分类器判断属于哪一类；</li>
<li>对分好类的目标的Bounding box进行线性回归，输出精细修正过后的Tighter Bounding box。</li>
</ol>
<h4 id="5-引用"><a href="#5-引用" class="headerlink" title="5 引用"></a>5 引用</h4><p><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">R-CNN</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/28/ResNet模型学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://oyvr3xxmh.bkt.clouddn.com/17-11-4/36375047.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="少年游">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/28/ResNet模型学习/" itemprop="url">ResNet模型学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-28T19:44:02+08:00">
                2017-12-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/note/" itemprop="url" rel="index">
                    <span itemprop="name">note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="ResNet-深度残差网络"><a href="#ResNet-深度残差网络" class="headerlink" title="ResNet (深度残差网络)"></a>ResNet (深度残差网络)</h3><p>&emsp;&emsp;基于之前的一些模型的学习，可以看出卷积神经网络的深度对最后的分类和识别结果有着重要的影响。而且可以看到自2012年，每年ImageNet的ILSVRC冠军的网络模型的深度都在递增。ResNet是由KaiMing He在2015年发表，并基于该模型获得了当年ImageNet detection，ImageNet localization，COCO detection等多个比赛的冠军。</p>
<p>&emsp;&emsp;在ImageNet数据集中，He使用了一个152层的深度残差网络，虽然该网络的深度是VGG网络的八倍，但复杂度却更低。最后其在该数据集上top-5的错误率为3.57%，这是一个非常好的成绩（人类的平均错误率是5.1%），该篇论文主要的观点便是提出了一个残差结构。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h4><p>&emsp;&emsp;He在论文的开篇提出一个问题：Is learning better networks as easy as stacking more layers? 但事实是如果只是常规的去堆叠网络的话，当网络越来越深，效果反而会越来越差。其中一个重要的原因便是梯度消失/爆炸现象，当网络变得越来越深后，准确率便趋于饱和，然后继续增加准确率反而开始下降，而且这个下降也并不是由过拟合引起的，如下图便证实了这一点，一个常规的网络在深度达到56层后的效果还不如26层。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/32524482.jpg"><br>&emsp;&emsp;基于此，He提出一种解决方案：针对一个较浅的网络在为其添加更多的层时，添加的层是恒等映射，其它层是从学习后的较浅模型拷贝过来的。这样的解决方案能使较深的模型不至于产生比较浅的模型更高的误差。</p>
<p>&emsp;&emsp;在论文中，通过引入一个深度残差学习框架来解决退化问题（degradation，就是指非常深的网络的性能反而比较浅的网络差，而且越深越差）。通过拟合残差映射来代替之前的拟合底层映射，假设我们最后要求的映射为H(x)，但求H(x)并不容易，我们转而去求F(x)≔H(x)-x，然后再通过F(x)+x来求解最后的H(x)。</p>
<p>&emsp;&emsp;在这里可能很多人会对H和F函数感到不理解，在这里引用一下知乎上关于这两个函数的回答：F是求和前的网络映射，H是从输入到求和后的网络映射。比如把5映射成5.1，那么引入残差前是F(5)=5.1，引入残差后是H(5)=5.1,F(5)=H(5)-5=0.1。引入残差后对输出的变化更加敏感，比如原来是从5.1增加到了5.2，映射F的输出增加了2%。而对于残差结构的网络，映射F是从0.1增加到了0.2，增加的是100%。残差的思想便是去掉相同的主体部分，从而突出微小的变化，可以理解为差分放大器。如下是残差结构，称之为residual block。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/18-1-13/70337038.jpg"><br>&emsp;&emsp;F(x)+x可以被认为是一个“shortcut connections”，关于shortcut我们可以看下图：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/34147465.jpg"><br>&emsp;&emsp;在上图中a^([l])到a^([l+2])经过了两层神经网络层，在传统的神经网络的前向传播中，a^([l])首先经过线性操作得到z^([l+1])=w^([l+1]) a^([l])+b^([l+1])，并通过ReLU非线性激活得到a^([l+1])。然后又通过线性操作得到z^([l+2])=w^([l+2]) a^([l+1])+b^([l+2])，同样的ReLU非线性激活得到a^([l+2])。这是常规网络在前向传播中所进行的处理。而shortcut的处理则是它首先将a^([l])复制一份到z^([l+2])之后，然后和z^([l+2])一起进行一个ReLU非线性激活。</p>
<p>&emsp;&emsp;论文中还提到了该模型随着深度的增加，产生的结果要比之前传统的网络要表现好得多。而且该模型不仅在ImageNet的数据集上表现的很好，在诸如COCO等的数据集上同样有着很好的表现，说明该模型可以被用作一个通用的模型。</p>
<h4 id="2-深度残差学习"><a href="#2-深度残差学习" class="headerlink" title="2 深度残差学习"></a>2 深度残差学习</h4><h5 id="2-1-残差学习和shortcuts恒等映射"><a href="#2-1-残差学习和shortcuts恒等映射" class="headerlink" title="2.1 残差学习和shortcuts恒等映射"></a>2.1 残差学习和shortcuts恒等映射</h5><p>&emsp;&emsp;残差学习使一个residual block的网络层去近似一个残差函数F(x)≔H(x)-x来代替原本的H(x)，并用F(x)+x来得到H(x)。尽管两种形式最终都能近似拟合成想要的那个函数，但两种方法学习的困难度可能是大有不同的。</p>
<p>&emsp;&emsp;这种重构的想法来自于，随着深度增加而引发的退化问题这种有悖常理的现象，正如开始中提到的那样，如果增加的网络做的是一个1:1的恒等映射的话，那么深层次的网络没有理由错误率会比浅层的网络高，所以表明了多层非线性函数激活层的叠加最后对拟合函数出现了问题。通过残差学习的结构，如果恒等映射是最优的话，求解器可以简单地将多层非线性连接的权重推向零从而接近恒等映射。</p>
<p>&emsp;&emsp;对于shortcut connection的引入，它既没有增加多余的参数也没有增加多余的计算复杂度。这一点不仅在实验过程中非常有用而且也是该网络相比于常规网络有更好表现的重要点。有一点需要注意的是，因为shortcut最后会将a^([l])同z^([l+2])相加，所以必须要保证这两个参数的维度是一致的。我们可以从上面的图中看到一个residual block中间是有两层，在这里两层三层甚至更多层都是可以的。但是如果只有一层的话，那y=W_1 x+x，这样做观察不到有任何的优势。还有一点就是，该图中的两层网络都是全连接层网络，这种结构同样适用于卷积神经网络，也就是说这两个全连接网络也可以替换为两层卷积神经网络。</p>
<h5 id="2-2-网络架构"><a href="#2-2-网络架构" class="headerlink" title="2.2 网络架构"></a>2.2 网络架构</h5><p>&emsp;&emsp;ResNet传统的网络结构是基于VGG网络的基准。VGG网络的特点主要是在卷积层大量使用了3x3的卷积核，并且基于两个简单的设计规则：1.针对相同尺寸的输出特征图，在输出之前所采用的卷积层中每一层filters的数目是相同的；2.如果特征图的尺寸在经过池化层后减半，那么相应的在之后卷积层的每一层中filters的数目增加一倍，以此来保证每层的时间复杂度。ResNet在最后全连接层与VGG网络的一个不同点在，VGG网络是在最后的flatten操作中将上一层max pooling得到的结果经过两个4096个神经元的全连接层最后以一个softmax预测1000个分类，而ResNet则是通过了一个avg pooling层直接跟上一个1000的softmax全连接层进行预测（这是在GoogLeNet中所提到的方法）。</p>
<p>&emsp;&emsp;在这里介绍一下用一个avg pooling加softmax代替几个全连接层加softmax的好处，首先一点原来全连接层叠加参数量和计算量极大，有时候一个网络的80%-90%的参数都集中在最后几个FC层中（比如VGG网络中第一个FC层的参数就是7x7x4096x512，接近1亿个参数），而ResNet中的替代方案的参数只有（1x1x512x1000，51.2万个参数）；第二点参数一多就容易造成过拟合，这样就导致原来的模型泛化能力很弱；最后一个好处是利用了空间信息，对于图像在空间中的变化更加鲁棒，下面是VGG和ResNet的架构对比图：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/52196762.jpg"><br>&emsp;&emsp;上图中间和右图的区别在于增加了shortcut connection，针对输入和输出维度相同时，该结构能被直接使用（右图实线部分）。而对维度增加的情况（右图虚线部分），因为shortcut传输过来的数据维度和没有经过shortcut的数据维度不一致（因为用了pooling层），论文中考虑两种选项：1.shortcut仍旧执行恒等映射，用全是0的数据来填充增加维度；2.通过使用1x1卷积核来改变维度使两者维度能一致。</p>
<h5 id="2-3-实现"><a href="#2-3-实现" class="headerlink" title="2.3 实现"></a>2.3 实现</h5><p>&emsp;&emsp;在针对ImageNet数据的实现过程中，每张输入图片的尺寸被重置为[256, 480]之间的随机数，以此来增加数据集，然后和其他网络一样的操作便是针对该图片，或将此图片经过垂直翻转后的图片进行一个224x224的裁剪。在RGB数据的操作上也使用了之前网络的数据增加方法（即改变对比度或颜色等）。训练过程中权重是重头开始训练的，使用了SGD和一个256大小的mini-batch，学习速率起始为0.1当准确率不再增加之后下降十倍。还有一点是ResNet中没有使用dropout，使用了0.0001的权重衰减和0.9的momentum。测试阶段用了10张裁剪的图片进行平均结果输出，同样的测试图片的尺寸也被重置为{224,256,384,480,640}等。</p>
<h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h4><p>&emsp;&emsp;在实验的过程中，首先采用了一个18层和一个34层的网络进行训练和测试，两个网络有着相同的形式，具体的结果如下图：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/63990330.jpg"><br>&emsp;&emsp;从上面的左图可以看到网络退化现象，一个34层的常规网络最终的训练误差要高于一个18层的网络，尽管这个18层网络可以说是它的一个子网络。He认为优化难度不大可能是由于梯度消失造成的，他们验证了前向传播和反向传播过程中信号都没有消失。推测深层的常规网络可能有指数级低收敛的特性，从而影响了训练误差。</p>
<p>&emsp;&emsp;在右图中可以看到用ResNet来进行测试，网络架构用的还是一样只不过多了一个shortcut connection。在第一次对比中使用了完全映射，针对维度不一致的问题采取的是通过填充0来保证维度一致，所以相比于左图的网络右边的网络没有增加任何一点参数。可以看到最终四个网络的训练误差如下表所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/96736967.jpg"><br>&emsp;&emsp;没有增加一个参数防止了网络退化问题得到发生，这里便可以窥到shortcut的强大之处。还有一点是，可以在上表中看到18层的常规网络和18层的ResNet的错误率几乎没有差别，但是ResNet会收敛地更加快。</p>
<p>&emsp;&emsp;接着论文中比较了Identity shortcut和projection shortcut两种处理维度不一致的方法。从上面的介绍中我们已经知道了这两种方法一个是用0填充参数，另一个是通过1x1卷积核改变维度。比较结果如下表所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/4087033.jpg"><br>&emsp;&emsp;在这里一共采用了三种方法（可以看到上表中也有A、B、C），第一种零填充方法，第二种是针对维度增加的那几层采取projection shortcut，而其它层仍然采用identity shortcut，第三种是所有的shortcut都是projection shortcut。从上表中可以看到三种方法最后的效果，B和C要好于A，C轻微地好于B，但是这三种方法之间的差别很小，说明projection shortcut其实并不是必要的，而且Identity shortcut还能减少一些计算和内存占用。</p>
<p>&emsp;&emsp;接下去说一个瓶颈结构（Deeper Bottleneck Architectures），如下图ResNet将shortcut中间的两层变为了三层（右图所示）：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/75506036.jpg"><br>&emsp;&emsp;在这个结构中用到了两个1x1卷积层，第一个1x1卷积层所做的操作是降维，如上图中便是将256维降到了64维，然后通过一个3x3卷积层获取特征，第二个1x1卷积层所做的是升维，将输出又变回与输入一致，这样做就可以大大地减少计算复杂度，而且还能保证性能没有什么损失，这种结构很像一个瓶颈的结构：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-28/93066085.jpg"><br>&emsp;&emsp;论文之后还介绍了ResNet在CIFAR-10数据集上的表现，同样证明了ResNet不会出现像常规网络那样随着深度增加的退化现象。</p>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h4><p>&emsp;&emsp;ResNet网络的重点如下：</p>
<ol>
<li>Residual block和shortcut connection的引入，使网络能够达到更高的层数并且不会发生网络退化现象；</li>
<li>再一次（GoogLeNet也提到过）证明了用avg pooling+softmax代替几个FC+softmax能带来更小的参数数量和计算量并不容易造成过拟合。</li>
</ol>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="external">ResNet</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/20/GoogLeNet模型学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://oyvr3xxmh.bkt.clouddn.com/17-11-4/36375047.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="少年游">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/20/GoogLeNet模型学习/" itemprop="url">GoogLeNet模型学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-20T16:53:38+08:00">
                2017-12-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/note/" itemprop="url" rel="index">
                    <span itemprop="name">note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="GoogLeNet-Inception-Net"><a href="#GoogLeNet-Inception-Net" class="headerlink" title="GoogLeNet(Inception Net)"></a>GoogLeNet(Inception Net)</h3><p>&emsp;&emsp;GoogLeNet是2014年ImageNet Classification的冠军得主，该架构的主要特点是充分地利用了网络内部的计算资源，并在设计上精雕细琢是的最终的层数达到了22层。其架构的决定基于Hebbian理论（维基的解释：我们可以假定，反射活动的持续与重复会导致神经元稳定性的持久性提升……当神经元A的轴突与神经元B很近并参与了对B的重复持续的兴奋时，这两个神经元或其中一个便会发生某些生长过程或代谢变化，致使A作为能使B兴奋的细胞之一，它的效能增强了）。</p>
<p>&emsp;&emsp;2014年是卷积神经网络的绽放年，对比之前介绍过的同一年出现的VGG网络，GoogLeNet的一个非常大的优势便是其要训练的参数远小于VGG网络，故计算成本要低很多。论文中还说到其论文名字中的deep有着两层的含义，其一是引入了了一种新的层次组织方式(Inception module)；其二是更直观地便是加深了网络的深度。</p>
<h4 id="1-相关工作"><a href="#1-相关工作" class="headerlink" title="1 相关工作"></a>1 相关工作</h4><p>&emsp;&emsp;在这里主要对1x1卷积层进行介绍，原文中描述了在1x1卷积层后跟上ReLU层，使当前的网络能轻易地集成该卷积层。所以在该网络中大量地采用了1x1的卷积核，其主要目的是对数据进行了降维，从而移除计算的瓶颈。这样做不仅能增加网络的深度而且在保证网络的宽度的同时对最终的性能没有明显的损失。</p>
<p>&emsp;&emsp;先说说降维，因为3x3和5x5的卷积核在filter数目大的时候对前一层网络进行卷机操作时相当的耗时，所以通过在两者之间加入1x1卷积核将维度降下来。比如一张100x100且channels为100的图片经过20个filter的1x1卷积层后维度就降到了100x100x20。当然既然能降维，该卷积层同样能做到升维，所以其中一个重要的作用便是调节维度；另一个好处便是1x1卷积层后面直接可以加入ReLU非线性层，增加模型的复杂度（这个在VGG的网络中也采用过）。</p>
<p>&emsp;&emsp;论文中还提到了R-CNN将物体检测分为了两个步骤，首先是利用颜色和纹理等低级信号，以先不管类别的方式去产生目标位置的候选区域；然后利用CNN分类器去上述的区域中识别对象。</p>
<h4 id="2-动机和高层次的思考"><a href="#2-动机和高层次的思考" class="headerlink" title="2 动机和高层次的思考"></a>2 动机和高层次的思考</h4><p>&emsp;&emsp;GoogLeNet同样也提出了想直截了当地增加最后神经网络的性能的一个方法便是增加网络的深度（该观点在VGG也同样地提出过），但深度的增加也存在着两个弊端：</p>
<ul>
<li>其一，更深的网络意味着更多的参数，这样会导致网络更容易地出现过拟合的现象，尤其是当训练的带标记的数据有限。这将会导致网络出现瓶颈，因为高质量的带标记的训练数据的获得是非常棘手和昂贵的；</li>
<li>其二，一味地增加深度同样会极大地增加计算资源。比如在一个较深的神经网络中，如果有两层卷积层是相互连接的，那么它们之中filters数目的增加将会导致计算量的平方式的增加。而且当增加的能力使用率很低时（比如大多数的权重在最后都会趋于0），那么很大的计算资源就被浪费了。</li>
</ul>
<p>&emsp;&emsp;而如何处理这两个弊端呢？一个比较好的方法便是将全连接层甚至是卷积层以稀疏连接层代替，这个想法来自Arora等的工作，他们的主要成果表明了如果以一个大型的稀疏神经网络来表示数据集的概率分布，那么通过分析上一层激活后的相关统计信息并聚集输出高度相关的神经元，便可以一层一层地构建最优的网络拓扑结构。</p>
<p>&emsp;&emsp;因为计算机对非均匀的稀疏数据的计算性能很差，早先的网络采用了随机的系数连接来打破网络的对称性提高学习的速率，因此在AlexNet中又重新地采用了全连接层来达到更好地并行计算能力。所以现在的主要问题便是是否存在这样的一个中间步骤，既能够充分地利用网络的稀疏结构，也能够使计算机对密集矩阵的高计算性能得到使用。大量的文表明了在稀疏矩阵的计算上可以通过将它们聚类为一个相对于密集的子矩阵从而使现有的技术的实际性能运用到矩阵运算中，Inception便是基于此提出了Inception module。</p>
<h4 id="3-架构细节"><a href="#3-架构细节" class="headerlink" title="3 架构细节"></a>3 架构细节</h4><p>&emsp;&emsp;Inception架构的主要思想便是如何在卷积网络中找出一个近似的局部最优的稀疏结构，且该结构能被密集组件覆盖，得到这么一个结构后便能将其在空间中不断地重复最后构建整个网络。因此根据Arora等人提出的层次结构，将filter组成一个filter bank构成一层。目前Inception的架构的filter的尺寸被限制为1x1，3x3和5x5的，这个是基于便利性考虑的。同样因为池化层操作在目前大多数网络中都有着不错的效果，所以在该结构中同样增加一条并行的池化线，作者首先提出了如下的结构：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-20/7901825.jpg"><br>&emsp;&emsp;如上的Inception模块在前一层之后跟着三个不同大小的卷积核以及一个max pooling核。采用了不同大小的卷积核意味着能得到不同的感受野，而针对每种卷积核最后只需要分别设置不同的pad便能得到相同维度的特征并进行拼接。而随着网络的深度加深，所需要捕获更高层次的抽象信息，那么就要使3x3和5x5卷积核的比例在深网络中加大。</p>
<p>&emsp;&emsp;但该组件的一个很大的问题便是，即使是5x5卷积核的数量不都，当filters数目很大时其参数的代价是过于昂贵的（比如上一层的输出是100x100x64，那这一层的输出是100x100x128，那么光一个5x5的卷积核所需要的参数便是64x5x5x128）。这个问题在加入了池化组件后会变得更加的明显，因为输出的filter的数量是等同于上一阶段filter的数量(即卷积核的数量)，而池化层和卷积层的合并势必会导致这一阶段到下一阶段输出的增加（因为在加入池化层后缩小了数据的长宽的维度，必定会增加其高度的维度，而卷积层的filter数目必须得和池化的filter数目保持一致）。所以尽管这种结构可能会覆盖最优的稀疏结构，但是随着阶段的增加其计算量过于庞大。</p>
<p>&emsp;&emsp;为了解决上面描述的问题，论文中说到需要先进行信号和信息的压缩对数据进行降维处理，然后再传入3x3和5x5的卷积核，所以在这里使用了1x1的卷积核对数据先进行降维处理，具体结构如下所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-20/98447924.jpg"><br>&emsp;&emsp;从上图可以看到上一层的输出先经过1x1的卷积核进行处理再传入大的卷积核中，比如上一层的输出为100x100x64，如果先经过32个1x1的卷积核处理再传入128个5x5x32的卷积核中最后同样输出的也是100x100x128的数据，但是其参数的总量大约减少了四倍。该结构还有一个好处便是，它能是下一阶段的卷积层以不同的感受野去提取到上一阶段的特征并将它们整合在一起。</p>
<h4 id="4-GooLeNet"><a href="#4-GooLeNet" class="headerlink" title="4 GooLeNet"></a>4 GooLeNet</h4><p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-20/29711383.jpg"><br>&emsp;&emsp;上表展示了在当时比赛中提交的最成功的一个例子，下面将详细分析下上表。在该网络中的所有卷积层、池化层后均采用的是ReLU作为激活函数，该网络的输入感受野是224x224x3,3为RGB的三条通道，输入数据也只经过平均化的处理（即使RGB数值以0为中心，位于-1到1之间）。</p>
<p>&emsp;&emsp;“#3x3 reduce”和“#5x5 reduce”代表了降维所用的1x1的卷积核的个数。比如看表的第五行（inception（3a）），上一层的输出是28x28x192，Inception module中直接经过1x1的卷积核后输出的channels是64，经过3x3的卷积核时先用1x1卷积核把192维降到了96维然后经过3x3卷积核输出的channels是128,5x5的卷积核是先降维到32，然后输出的channels是32，pooling直接输出的channels是32，这样全部合并后是256维，所以通过这个inception module后输出的数据size是28x28x256。</p>
<p>&emsp;&emsp;GoogLeNet架构中还有几个需要注意的点是：</p>
<ul>
<li>在最后的输出层之前采用Average Pooling代替全连接层,该做法使性能提升了0.6%；</li>
<li>还是在Average Pooling后增加了一个全连接层只为了以后进行微调；</li>
<li>尽管将全连接层移除，但dropout仍旧还是被使用，dropout值为0.7；</li>
<li>因为层数比较深为了防止发生梯度消失，增加了两个辅助分类器来进行反向传播，可以从下图中看到该结构中总共有三个softmax，这两个辅助分类器分别位于Inception（4a）和Inception（4d）后面，在实际测试的时候这两个分类器将不使用。<br><strong>下面为完整的GoogLeNet架构图：</strong></li>
</ul>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-20/31510768.jpg"><br>&emsp;&emsp;该模型在训练的过程采用了异步的随机梯度下降算法，并设置了0.9的momentum值，针对学习率设置采用的是每经过8个epochs学习速率减小4%。此外还用到了诸如Polyak平均、光度失真、随机插值等方法。</p>
<p>&emsp;&emsp;在进行测试的过程中，论文采取了如下几种方法：</p>
<ol>
<li>独立地训练了7个版本的模型，这七个模型初始化的权重和学习率均相同，唯一的区别便是下采样采取的方法和它们从输入中获得的感受野的随机性不同;</li>
<li>针对测试集，采取了一些和AlexNet中提到的方法类似的方法，首先将原始测试图片的宽或高分别resize成256,288,320,352四种，并对这些长方形图片分别从左、中、右裁剪出三个正方形（肖像图是上中下）。然后对这些裁剪的图分别取左上、左下、右上、右下、中心以及整张图片，将上述六种图片均resize成224x224，最后将上述处理完后的图片再做一次镜像翻转，然后作为测试集输入。经过这样的处理，一张图片便可以得到4x3x6x2=144张裁剪图，最后softmax输出的结果是平均这些图片得到的结果。</li>
</ol>
<h4 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h4><p>&emsp;&emsp;GoogLeNet网络的重点如下：</p>
<ol>
<li>Inception module的提出，使网络结构既保证稀疏性，又利用了密集矩阵的高计算性能;</li>
<li>1x1卷积核的使用，在该网络中1x1卷积核的使用，降低了特征的维度，最后极大地减少了参数数量和运算量；</li>
<li>后的分类器部分只采用了一个average pooling层和一个全连接层（其实全连接层也可以去掉），也在一定程度上降低了运算的数量。</li>
</ol>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="external">GoogLeNet</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/18/VGG模型学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://oyvr3xxmh.bkt.clouddn.com/17-11-4/36375047.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="少年游">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/18/VGG模型学习/" itemprop="url">VGG模型学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-18T15:30:19+08:00">
                2017-12-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/note/" itemprop="url" rel="index">
                    <span itemprop="name">note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>&emsp;&emsp;VGG网络由牛津大学提出，并发表于2015年的ICLR会议上，该论文针对每一个卷积层采用了一个3x3的卷积核对图像做卷积，并基于当时技术的配置将卷积神经网络的层数推向了16-19层。VGG网络在2014年的ImageNet比赛中获得了物体检测的冠军以及图像分类的亚军。</p>
<h4 id="1-ConvNet配置"><a href="#1-ConvNet配置" class="headerlink" title="1 ConvNet配置"></a>1 ConvNet配置</h4><h5 id="1-1-架构"><a href="#1-1-架构" class="headerlink" title="1.1 架构"></a>1.1 架构</h5><p>&emsp;&emsp;VGG-16网络唯一所做的预处理便是将输入的224x224图片的每个像素减去RGB的均值（这个操作是使得输入的各个维度的数据都能中心化到0，还可以针对每个数据除以rgb最大值255使所有输入数据均处于-1-1之间）。在该网络中使用了3x3的卷积核（感受野为3x3），这是一个和之前LeNet5以及AlexNext相比非常小的卷积核，同时在该网络中还用到过1x1的卷积核（这个可以看做是对输入通道的线性变化）。卷积核的stride设置为1，padding的设置为same，池化操作为2x2的窗口，步长为2。最后的全连接层分别为4096、4096、1000通道，这和之前的网络相比没什么区别，此外采用的激活函数也仍旧是ReLU。VGG-16网络还有一个操作是它不再使用AlexNet中提到过的LRN，因为它觉得这个方法对准确率的提升不明显反而增加了内存的消耗和计算的时间，下面是简化版的VGG-16架构：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-18/16448630.jpg"></p>
<h5 id="1-2-配置"><a href="#1-2-配置" class="headerlink" title="1.2 配置"></a>1.2 配置</h5><p>&emsp;&emsp;在该论文中，该卷积网络的配置如下表所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-17/67520009.jpg"><br>&emsp;&emsp;从上表中可以看到总共有六个网络（A-E），它们之间除了层数的不同以外，其它的诸如stride等参数全部一致。每一层的channels也是相对较小的，从第一层的64每次增加一倍到最后达到512，下表展示了这些网络的参数个数总数，可以看出尽管层数增加了很多，但总的参数的数量并没有增加很多。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-17/39393803.jpg"><br>&emsp;&emsp;可以容易地看到2个3x3的卷积核的感受野大小和一个5x5的卷积核感受野大小相似，3个3x3的卷积核的感受野同一个7x7的卷积核的感受野相似。那么相比于一个7x7的卷积核，3个3x3的卷积核堆叠有什么好处？其一：3个3x3的卷积核对应三层卷积层，能比一层卷积层多出两个非线性层处理，使得最终的模型更加复杂，辨别力更强；其二：参数数量缩减，以前一个7x7的卷积核（假设有C的通道数），那么参数的数量是49C^2，而现在的参数是3（3^2 C^2 ）=27C^2。在表1中的1x1卷积核的本质上是将输入进行线性投影后加入一个非线性层。</p>
<h4 id="2-训练和测试"><a href="#2-训练和测试" class="headerlink" title="2 训练和测试"></a>2 训练和测试</h4><p>&emsp;&emsp;训练的过程基本和AlexNet的训练过程类似，使用了随机梯度下降（基于反向传播）并将momentum值设置为0.9，mini batch的值为256，dropout参数设置为0.5，权值衰减的措施也如AlexNet一样，初始值为0.01每当validation数据集的准确率基本不变的时候减小十倍。最终该模型迭代了370000次（74epochs），尽管参数的数量和网络的层数都要大于AlexNet但最终只需要更少的epochs，推测原因应该是：1.更大的层数和更小的卷积核导致的隐式正则化；2.对每层的参数初始化采取了正确的方法。<br>&emsp;&emsp;VGG网络的权重参数初始化是首先使用表1中的A网络，用随机的参数训练该网络至收敛，然后将训练完成的参数去初始化剩下B-E网络的前四层卷积层和最后的三层全连接层，其余层仍旧使用随机参数。<br>&emsp;&emsp;在该篇论文中用了很多篇幅去强调了网络输入图片的尺寸问题，对于训练的图片采取把图片全部转换成边长为S的正方形（如S=256便是AlexNet所采用的尺寸，还有S=384等），然后对转化好的图片随机裁剪224x224的部分去训练。该模型采取了两种方法去设置S，</p>
<ol>
<li>单尺度训练：先把所有图片设置为256x256然后裁剪后进行训练得到训练好的模型，接着再把所有图片设置为384x284，用S=256时得到的参数去初始化S=384时候的参数，并把学习速率调低十倍；</li>
<li>多尺度训练：所有图片的尺寸并不是唯一的，即它们是随机于〖[S〗_min,S_max]（S_min=256，S_max=512），这样的话就会使每张图片的尺寸不尽相同，然后仍然是对图片进行裁剪将裁剪后224x224的图片作为训练集进行训练，（原文说因为图片中物体的大小有不同的size，所以这样做能把这点考虑进去，而且是通过尺寸的抖动增大了数据集），同样这里采取了S=384训练好后的模型参数来进行初始化，并设置学习速率为0.001。</li>
</ol>
<p><strong>网络的测试过程中，针对测试图片的scale处理也采取了两种方法：</strong></p>
<ol>
<li>dense evaluation：即全连接卷积网络，针对的是整张训练图片。在这里同样采取了图片的水平翻转增加了测试集，最后的结果由原始图片和翻转图片的结果取平均</li>
<li>multi-crop evaluation：即针对测试图片进行多重的裁剪，这样会使计算量变得非常大，因为模型要针对每一个crop进行计算。论文中还提到了这两种方法之间可以进行互补，原因呢？两种方法有着不同的卷积边界条件（这里读论文的时候读的迷迷糊糊的）。</li>
</ol>
<h4 id="3-分类实验"><a href="#3-分类实验" class="headerlink" title="3 分类实验"></a>3 分类实验</h4><p>&emsp;&emsp;ImageNet的classification最终结果由两个指标来进行评估，分别是top-1和top-5 error，前者是多类分类误差即不正确的分类图像占据的比例；后者是ILSVRC采用的主要评估标准，它是在前五个中都不是该图片类别的占比（即预测出来的前五个类别都不是该图片的类别所占总数的比例）。</p>
<h5 id="3-1-单尺度评估vs多尺度评估"><a href="#3-1-单尺度评估vs多尺度评估" class="headerlink" title="3.1 单尺度评估vs多尺度评估"></a>3.1 单尺度评估vs多尺度评估</h5><p>&emsp;&emsp;单尺度评估即设置测试数据集的图片大小唯一，即如果训练图片S固定，那么Q=S，如果训练图片S∈[S_min,S_max ]，那么Q=0.5(S_min+S_max)，最后的结果如下表所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-18/40366083.jpg"><br>&emsp;&emsp;从上表中能得到的结论：</p>
<ul>
<li>AlexNet中提到的LRN层没有任何作用；</li>
<li>随着层数的增加，错误率也在不断地减小；</li>
<li>1x1卷积核引入后起到了一些作用（CA网络比B网络表现好），但拿C网络和D网络相比，3x3卷积核还是性能要比1x1的要好；</li>
<li>当层数达到19层后，网络的识别率已经达到了饱和；</li>
<li>一个具有小卷积核的深网络要完全胜过一个大卷积核的浅网络；</li>
<li>S以区间随机形式设置的方式结果要明显地好于用一个固定值设置。</li>
</ul>
<p>&emsp;&emsp;下面再来看多尺度评估的结果，多尺度评估是测试集图片的大小也不再是唯一了，而是取三个值最后将三个值的结果取平均进行预测。取法如下：如果S的大小固定，那么Q={S-32，S,S+32}，如果S的大小为一个区间内，那么Q={S_min，0.5(S_min+S_max),S_max }，下表为结果：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-18/32921728.jpg"><br>&emsp;&emsp;该表的结果说明了多尺度的抖动效果要较优于单尺度。</p>
<h5 id="3-2-多网络融合"><a href="#3-2-多网络融合" class="headerlink" title="3.2 多网络融合"></a>3.2 多网络融合</h5><p>在进行实际的操作中，最终的分类结果是融合了好几个模型的结果进行平均化，结果如下：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-18/30597725.jpg"></p>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h4><p>VGG网络的重点如下：</p>
<ul>
<li>堆叠使用小卷积核（3x3卷积核）来代替大卷积核使网络变得更深更复杂；</li>
<li>网络越深对精度的提高越大；</li>
<li>但该网络的参数数量过于庞大搜索空间非常大。</li>
</ul>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="http://arxiv.org/abs/1409.1556" target="_blank" rel="external">VGG</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/16/AlexNet模型学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Li">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://oyvr3xxmh.bkt.clouddn.com/17-11-4/36375047.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="少年游">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/16/AlexNet模型学习/" itemprop="url">AlexNet模型学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-16T23:38:08+08:00">
                2017-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/note/" itemprop="url" rel="index">
                    <span itemprop="name">note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-LeNet5"><a href="#1-LeNet5" class="headerlink" title="1 LeNet5"></a>1 LeNet5</h3><p>&emsp;&emsp;LeNet5于1998年由Yang LeCun发布，从此深度学习这个领域开始为人所知，而在这个网络架构中很多东西在现如今的架构中也一直沿用着，比如卷积层（Convolutional Layer）加池化层（Polling Layer）加非线性激活层（Fully Connected Layer）这种CNN经典的组合也正是在这篇论文中提出的。</p>
<p>&emsp;&emsp;因为当时的技术有限，并没有如今的算力，无法采用GPU来进行运算，即使是CPU的运算也相当缓慢。相较于今天各种网络，这个五层网络是一个非常小的网络，只有大约60,000参数需要训练，而在现在你所看到的一些常见网络中，参数无不是从千万到亿级别的，如下为LeNet5架构：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-15/91909246.jpg"><br>&emsp;&emsp;当时LeCun是针对黑白的手写数字图片进行识别，如下图所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-16/30031856.jpg"><br>&emsp;&emsp;在这里我们可以看到输入的图片为一个32x32像素的黑白图片，所以chanel为1，然后经过一个5x5的convolutional filter进行卷积之后再经过avg pooling层，重复两次最终跟上两个个全连接层进行输出，在当时并没有采用softmax function进行输出，当时所采用的输出方法现已经淘汰，这里我们一般都会采用softmax function进行输出。</p>
<p>&emsp;&emsp;LeNet5所做的工作是一个开创性的工作，其提出了CNN的三大特性：1.局部感知（卷积核）；2.下采样（池化）；3.权值共享，总得来说正是LeNet5架构把人们带入了深度学习的领域。</p>
<h3 id="2-AlexNet"><a href="#2-AlexNet" class="headerlink" title="2 AlexNet"></a>2 AlexNet</h3><p>&emsp;&emsp;在1998-2012这十多年间，神经网络的发展异常的缓慢，而2012年由Alex Krizhevsky发表的AlexNet以绝对的优势赢得了当年的ImageNet竞赛的第一，技惊四座，也正是从此深度学习和神经网络又大范围地回到人们的视野。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-16/78152235.jpg"><br>&emsp;&emsp;上图为ImageNet classification每届冠军队伍最后的错误率，我们可以看到在2012年神经网络及AlexNet的出现将错误率硬生生地下降了几乎10%，这在之前看来提升速度是非常夸张的，接着来看AlexNet的7层架构图：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-15/82690531.jpg"><br>&emsp;&emsp;没错，这个架构和LeNet5非常地相似，两者之间的主要区别点在于首先AlexNet将层数提升到了7，其次将LeNet5中的Avg Pooling层均换成了Max Pooling层，再有在LeNet5那个年代激活函数用的均是sigmoid function或tanh function，而AlexNet则采用了如今依然非常有用的ReLU function，还有一些诸如过拟合的处理、多GPU等下面将选择部分讲述。</p>
<p>&emsp;&emsp;该论文数据集选用的为ILSVRC-2010，这个数据集包含了1000中类别，每个类别大致有1000张左右的图片，120万的training dataset，50000的validation dataset和150000的testing dataset。</p>
<h4 id="2-1-ReLU-Rectified-Linear-Units-激活函数"><a href="#2-1-ReLU-Rectified-Linear-Units-激活函数" class="headerlink" title="2.1 ReLU(Rectified Linear Units)激活函数"></a>2.1 ReLU(Rectified Linear Units)激活函数</h4><p>&emsp;&emsp;传统的神经网络大多采用f(x)=tanh⁡(x)或者f(x)=〖(1+e^(-x))〗^(-1)作为激活函数，在进行梯度下降的训练过程中，这些饱和非线性函数比非饱和非线性函数f(x)=max⁡(0,x)训练的要慢得多。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-15/35591579.jpg"><br>&emsp;&emsp;如上图所示，实线代表ReLU激励函数，虚线代表tanh激励函数，在同样的神经网络下，ReLU只需要6次迭代即可达到25%的训练错误率，而tanh则需要35次以上的迭代次数，这里就可以看出ReLU在对网络越大，参数越多的情况下训练速度的提升是非常明显的。</p>
<h4 id="2-2-架构"><a href="#2-2-架构" class="headerlink" title="2.2 架构"></a>2.2 架构</h4><p>&emsp;&emsp;从AlexNet的架构图以及论文的描述中我们可以得到AlexNet的具体细节如下图所示：</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-15/52657695.jpg"><br>&emsp;&emsp;可以看到AlexNet共有5层卷积层，其中前两个卷积层后均跟着一个最大池化层和正则层，之后连续跟了三个卷积层以及一个最大池化层，最后是三层全连接层。注意Response Normalization Layer这个是用来做归一化处理的，而且在所有的卷积层和全连接层都用到了ReLU函数，而且从图4中也能看出来上下两个分别是在两个GPU上运行。其中每一层的神经元的个数分别为253440-186624-64896-64896-43264-4096-4096-1000，从这里我们能够看到绝大多数的神经元均集中在了卷积层，因为卷积层要对数据进行不断的升维。</p>
<h4 id="2-3-减少过拟合"><a href="#2-3-减少过拟合" class="headerlink" title="2.3 减少过拟合"></a>2.3 减少过拟合</h4><p>&emsp;&emsp;在神经网络的训练过程中，一直存在着两种问题：欠拟合和过拟合。AlexNet有大约6千万的参数要训练，同样存在着过拟合的问题，而针对过拟合AlexNet采取了两种方法来对抗。</p>
<h5 id="2-3-1-Data-Augmentation"><a href="#2-3-1-Data-Augmentation" class="headerlink" title="2.3.1 Data Augmentation"></a>2.3.1 Data Augmentation</h5><p>&emsp;&emsp;Data Augmentation的意思是数据增强，这是一个针对过拟合很常用的方法，既然模型过拟合，那么加大数据的输入量能在很大程度上减缓过拟合。而AlexNet在数据增强这方面的做法便是对输入的256x256的图片随意截取其224x224像素的部分，标签仍然是原来的标签，这样直接将训练数据集的大小增加了2048倍。而在针对测试集进行预测的时候，首先取出图片四个角落和中心被裁剪过的224x224图片，再加其余任意裁剪后的224x224五张图，共十张图进行预测，最后将10个结果取平均来得到最终的结果。另一个增强数据量的方法是改变每张图片的RGB值来得到更多的图片（这里采用了PCA），这样做最后的结果也表明了图像对比度和颜色的变化对图像本身的识别是不会产生影响的。</p>
<h5 id="2-3-2-Dropout"><a href="#2-3-2-Dropout" class="headerlink" title="2.3.2 Dropout"></a>2.3.2 Dropout</h5><p>&emsp;&emsp;Dropout是近几年非常流行的一个处理神经网络过拟合方法，在AlexNet中第二种处理过拟合便采用了dropout。Dropout的原理便是，设置一个dropout值，这样神经网络每层将有一定比例的神经元在当前epoch失效。</p>
<p><img src="http://oyvr3xxmh.bkt.clouddn.com/17-12-16/45381178.jpg"><br>&emsp;&emsp;如上图所示，设置dropout值为0.5，那么在每次的epoch中，每层的神经元将会有50%在神经网络的前向传播和后向传播中不在做出贡献，当然也可以针对每一层各自设置一个dropout值，实验表明该方法能有效地去对抗神经网络的过拟合。Dropout的作用减少了神经元之间的相互依赖，因为每一个epoch都无法保证上一趟epoch所依赖的神经元是否会在这次消失，这样使得每个神经云尽可能地学会更多特征。而也正是dropout的使用使得AlexNet极大地减轻了过拟合带来的损失，并使训练的次数翻倍。还有一点需要注意的是，dropout只需要用在训练的过程，而在针对测试数据集时是不能使用dropout的，那样做只会对你的预测增加噪声。还有一点需要注意的是，在对每一层使用完dropout后所得到的值，比如第三层共有30个神经元，设置dropout的值为0.8，这样有6个神经元在一趟epoch中被消除，而得到的结果需要除以dropout的值。即在这一层得到的值还需要都除以0.8后再传向下一层。</p>
<h4 id="2-4-训练细节"><a href="#2-4-训练细节" class="headerlink" title="2.4 训练细节"></a>2.4 训练细节</h4><p>&emsp;&emsp;AlexNet模型采用了随机梯度下降（SGD），设置mini batch的大小为128，momentum参数为0.9（momentum的作用是为了加速随机梯度下降在某一个方向上的搜索，并减少模型的震荡导致无法收敛），权值衰减设为0.0005（目的主要是为了防止过拟合），权值更新的公式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">v_(i+1)≔0.9∙v_i-0.0005∙ϵ∙ω_i-ϵ∙⟨∂L/∂ω│ω_i ⟩_(D_i )</div><div class="line"></div><div class="line">ω_(i+1)≔ω_i+v_(i+1)</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其中i是迭代参数，v为momentum参数，ϵ为学习速率，⟨∂L/∂ω│ω<em>i ⟩</em>(D_i )是目标函数对ω，在ω_i上的第i批微分D_i的平均。AlexNet学习速率的设置是手动调节的，起始的学习速率为0.01，当validation数据集的错误率不再变化的时候将学习速率调小十倍后继续训练。</p>
<h4 id="2-5-总结"><a href="#2-5-总结" class="headerlink" title="2.5 总结"></a>2.5 总结</h4><p>&emsp;&emsp;AlexNet模型的重点如下:</p>
<ol>
<li>使用了ReLU作为非线性激活函数;</li>
<li>使用了两种数据扩增的方式（裁剪和改变RGB的强度）;</li>
<li>将LeNet5的Avg Pooling改成了Max Pooling;</li>
<li>使用dropout来避免过拟合增大了迭代次数;</li>
<li>使用双GPU进行训练。</li>
</ol>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="external">LeNet5</a><br><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">AlexNet</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="http://oyvr3xxmh.bkt.clouddn.com/17-11-4/36375047.jpg"
                alt="Li" />
            
              <p class="site-author-name" itemprop="name">Li</p>
              <p class="site-description motion-element" itemprop="description">无病呻吟...</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  









<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>



  





  

  

  

  
  

  

  

  

</body>
</html>
